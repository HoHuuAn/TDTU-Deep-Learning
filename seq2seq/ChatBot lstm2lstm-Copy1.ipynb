{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d32b7d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a22e7f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\An\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "from tensorflow.keras import layers, activations, models, preprocessing, utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71843977",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('chatbot.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14fc196d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['have you read the communist', 'what is a government']\n",
      "['yes, marx had made some interesting observations.', 'ideally it is a representative of the people.']\n"
     ]
    }
   ],
   "source": [
    "questions = list(df['question'])\n",
    "answers = list(df['answer'])\n",
    "print(questions[:2])\n",
    "print(answers[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7310be14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOCAB SIZE : 1589\n"
     ]
    }
   ],
   "source": [
    "tokenizer = preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts( ['start']+questions + answers )\n",
    "VOCAB_SIZE = len( tokenizer.word_index )+1\n",
    "print( 'VOCAB SIZE : {}'.format( VOCAB_SIZE ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02dfa31e",
   "metadata": {},
   "source": [
    "Preparing data for Seq2Seq model\n",
    "This model requires 3 arrays encoder_input_data, decoder_input_data and decoder_output_data.\n",
    "\n",
    "For encoder_input_data: Tokensize the Questions and Pad them to their maximum Length.\n",
    "\n",
    "For decoder_input_data: Tokensize the Answers and Pad them to their maximum Length.\n",
    "\n",
    "For decoder_output_data: Tokensize the Answers and Remove the 1st element from all the tokenized_answers. This is the element which was added earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fdca1c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import re\n",
    "\n",
    "vocab = []\n",
    "for word in tokenizer.word_index:\n",
    "    vocab.append(word)\n",
    "\n",
    "def tokenize(sentences):\n",
    "    tokens_list = []\n",
    "    vocabulary = []\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.lower()\n",
    "        sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
    "        tokens = sentence.split()\n",
    "        vocabulary += tokens\n",
    "        tokens_list.append(tokens)\n",
    "        return tokens_list, vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01358c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(566, 15) 15\n",
      "[ 21   1  57   5 635   0   0   0   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "#encoder_input_data\n",
    "tokenized_questions = tokenizer.texts_to_sequences( questions )\n",
    "maxlen_questions = max( [len(x) for x in tokenized_questions ] )\n",
    "padded_questions = preprocessing.sequence.pad_sequences( tokenized_questions, maxlen = maxlen_questions, padding = 'post')\n",
    "encoder_input_data = np.array(padded_questions)\n",
    "print(encoder_input_data.shape, maxlen_questions)\n",
    "print(encoder_input_data[0])\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef49f500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(566, 34) 34\n",
      "[103 815 316 165  88 475 816   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "# decoder_input_data\n",
    "tokenized_answers = tokenizer.texts_to_sequences( answers )\n",
    "maxlen_answers = max( [ len(x) for x in tokenized_answers ] )\n",
    "padded_answers = preprocessing.sequence.pad_sequences( tokenized_answers , maxlen=maxlen_answers , padding='post' )\n",
    "decoder_input_data = np.array( padded_answers )\n",
    "print( decoder_input_data.shape , maxlen_answers )\n",
    "print(decoder_input_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0dca984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(566, 34, 1589)\n"
     ]
    }
   ],
   "source": [
    "# decoder_output_data\n",
    "tokenized_answers = tokenizer.texts_to_sequences( answers )\n",
    "for i in range(len(tokenized_answers)) :\n",
    "    tokenized_answers[i] = tokenized_answers[i][1:]\n",
    "padded_answers = preprocessing.sequence.pad_sequences( tokenized_answers , maxlen=maxlen_answers , padding='post' )\n",
    "onehot_answers = utils.to_categorical( padded_answers , VOCAB_SIZE )\n",
    "decoder_output_data = np.array( onehot_answers )\n",
    "print( decoder_output_data.shape )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6b9bec",
   "metadata": {},
   "source": [
    "# Step 4: Defining Encoder Decoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "539949a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\An\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 15)]                 0         []                            \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)        [(None, 34)]                 0         []                            \n",
      "                                                                                                  \n",
      " embedding (Embedding)       (None, 15, 200)              317800    ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)     (None, 34, 200)              317800    ['input_2[0][0]']             \n",
      "                                                                                                  \n",
      " lstm (LSTM)                 [(None, 200),                320800    ['embedding[0][0]']           \n",
      "                              (None, 200),                                                        \n",
      "                              (None, 200)]                                                        \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)               [(None, 34, 200),            320800    ['embedding_1[0][0]',         \n",
      "                              (None, 200),                           'lstm[0][1]',                \n",
      "                              (None, 200)]                           'lstm[0][2]']                \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 34, 1589)             319389    ['lstm_1[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1596589 (6.09 MB)\n",
      "Trainable params: 1596589 (6.09 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_inputs = tf.keras.layers.Input(shape=( maxlen_questions , ))\n",
    "encoder_embedding = tf.keras.layers.Embedding( VOCAB_SIZE, 200 , mask_zero=True ) (encoder_inputs)\n",
    "encoder_outputs , state_h , state_c = tf.keras.layers.LSTM( 200 , return_state=True )( encoder_embedding )\n",
    "encoder_states = [ state_h , state_c ]\n",
    "\n",
    "decoder_inputs = tf.keras.layers.Input(shape=( maxlen_answers ,  ))\n",
    "decoder_embedding = tf.keras.layers.Embedding( VOCAB_SIZE, 200 , mask_zero=True) (decoder_inputs)\n",
    "decoder_lstm = tf.keras.layers.LSTM( 200 , return_state=True , return_sequences=True )\n",
    "decoder_outputs , _ , _ = decoder_lstm ( decoder_embedding , initial_state=encoder_states )\n",
    "decoder_dense = tf.keras.layers.Dense( VOCAB_SIZE , activation=tf.keras.activations.softmax ) \n",
    "output = decoder_dense ( decoder_outputs )\n",
    "\n",
    "model = tf.keras.models.Model([encoder_inputs, decoder_inputs], output )\n",
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(), loss='categorical_crossentropy')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737894dd",
   "metadata": {},
   "source": [
    "# Step 5: Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a91bf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "WARNING:tensorflow:From C:\\Users\\An\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "12/12 [==============================] - 5s 54ms/step - loss: 7.3520\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 1s 47ms/step - loss: 6.9129\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 1s 49ms/step - loss: 6.0874\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - 1s 48ms/step - loss: 5.8639\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 1s 49ms/step - loss: 5.7927\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - 1s 48ms/step - loss: 5.7613\n",
      "Epoch 7/100\n",
      "12/12 [==============================] - 1s 50ms/step - loss: 5.7266\n",
      "Epoch 8/100\n",
      "12/12 [==============================] - 1s 49ms/step - loss: 5.7056\n",
      "Epoch 9/100\n",
      "12/12 [==============================] - 1s 49ms/step - loss: 5.6837\n",
      "Epoch 10/100\n",
      "12/12 [==============================] - 1s 52ms/step - loss: 5.6607\n",
      "Epoch 11/100\n",
      "12/12 [==============================] - 1s 51ms/step - loss: 5.6412\n",
      "Epoch 12/100\n",
      "12/12 [==============================] - 1s 50ms/step - loss: 5.6217\n",
      "Epoch 13/100\n",
      "12/12 [==============================] - 1s 50ms/step - loss: 5.6001\n",
      "Epoch 14/100\n",
      "12/12 [==============================] - 1s 53ms/step - loss: 5.5867\n",
      "Epoch 15/100\n",
      "12/12 [==============================] - 1s 50ms/step - loss: 5.5685\n",
      "Epoch 16/100\n",
      "12/12 [==============================] - 1s 49ms/step - loss: 5.5591\n",
      "Epoch 17/100\n",
      "12/12 [==============================] - 1s 49ms/step - loss: 5.5388\n",
      "Epoch 18/100\n",
      "12/12 [==============================] - 1s 51ms/step - loss: 5.5263\n",
      "Epoch 19/100\n",
      "12/12 [==============================] - 1s 49ms/step - loss: 5.4967\n",
      "Epoch 20/100\n",
      "12/12 [==============================] - 1s 51ms/step - loss: 5.4883\n",
      "Epoch 21/100\n",
      "12/12 [==============================] - 1s 50ms/step - loss: 5.4662\n",
      "Epoch 22/100\n",
      "12/12 [==============================] - 1s 49ms/step - loss: 5.4522\n",
      "Epoch 23/100\n",
      "12/12 [==============================] - 1s 52ms/step - loss: 5.4372\n",
      "Epoch 24/100\n",
      "12/12 [==============================] - 1s 48ms/step - loss: 5.4113\n",
      "Epoch 25/100\n",
      "12/12 [==============================] - 1s 51ms/step - loss: 5.3946\n",
      "Epoch 26/100\n",
      "12/12 [==============================] - 1s 51ms/step - loss: 5.3788\n",
      "Epoch 27/100\n",
      "12/12 [==============================] - 1s 48ms/step - loss: 5.3548\n",
      "Epoch 28/100\n",
      "12/12 [==============================] - 1s 49ms/step - loss: 5.3341\n",
      "Epoch 29/100\n",
      "12/12 [==============================] - 1s 49ms/step - loss: 5.3024\n",
      "Epoch 30/100\n",
      "12/12 [==============================] - 1s 50ms/step - loss: 5.2800\n",
      "Epoch 31/100\n",
      "12/12 [==============================] - 1s 50ms/step - loss: 5.2494\n",
      "Epoch 32/100\n",
      "12/12 [==============================] - 1s 52ms/step - loss: 5.2259\n",
      "Epoch 33/100\n",
      "12/12 [==============================] - 1s 52ms/step - loss: 5.2017\n",
      "Epoch 34/100\n",
      "12/12 [==============================] - 1s 50ms/step - loss: 5.1811\n",
      "Epoch 35/100\n",
      "12/12 [==============================] - 1s 51ms/step - loss: 5.1408\n",
      "Epoch 36/100\n",
      "12/12 [==============================] - 1s 48ms/step - loss: 5.1174\n",
      "Epoch 37/100\n",
      "12/12 [==============================] - 1s 49ms/step - loss: 5.0903\n",
      "Epoch 38/100\n",
      "12/12 [==============================] - 1s 50ms/step - loss: 5.0569\n",
      "Epoch 39/100\n",
      "12/12 [==============================] - 1s 52ms/step - loss: 5.0425\n",
      "Epoch 40/100\n",
      "12/12 [==============================] - 1s 52ms/step - loss: 5.0072\n",
      "Epoch 41/100\n",
      "12/12 [==============================] - 1s 50ms/step - loss: 4.9759\n",
      "Epoch 42/100\n",
      "12/12 [==============================] - 1s 52ms/step - loss: 4.9384\n",
      "Epoch 43/100\n",
      "12/12 [==============================] - 1s 50ms/step - loss: 4.9128\n",
      "Epoch 44/100\n",
      "12/12 [==============================] - 1s 53ms/step - loss: 4.8711\n",
      "Epoch 45/100\n",
      "12/12 [==============================] - 1s 54ms/step - loss: 4.8479\n",
      "Epoch 46/100\n",
      "12/12 [==============================] - 1s 56ms/step - loss: 4.8050\n",
      "Epoch 47/100\n",
      "12/12 [==============================] - 1s 53ms/step - loss: 4.7784\n",
      "Epoch 48/100\n",
      "12/12 [==============================] - 1s 49ms/step - loss: 4.7429\n",
      "Epoch 49/100\n",
      "12/12 [==============================] - 2s 146ms/step - loss: 4.7040\n",
      "Epoch 50/100\n",
      "12/12 [==============================] - 2s 147ms/step - loss: 4.6705\n",
      "Epoch 51/100\n",
      "12/12 [==============================] - 2s 142ms/step - loss: 4.6315\n",
      "Epoch 52/100\n",
      "12/12 [==============================] - 2s 147ms/step - loss: 4.6069\n",
      "Epoch 53/100\n",
      "12/12 [==============================] - 1s 124ms/step - loss: 4.5717\n",
      "Epoch 54/100\n",
      "12/12 [==============================] - 2s 140ms/step - loss: 4.5316\n",
      "Epoch 55/100\n",
      "12/12 [==============================] - 2s 149ms/step - loss: 4.5077\n",
      "Epoch 56/100\n",
      "12/12 [==============================] - 2s 137ms/step - loss: 4.4624\n",
      "Epoch 57/100\n",
      "12/12 [==============================] - 2s 141ms/step - loss: 4.4218\n",
      "Epoch 58/100\n",
      "12/12 [==============================] - 2s 137ms/step - loss: 4.4013\n",
      "Epoch 59/100\n",
      "12/12 [==============================] - 2s 142ms/step - loss: 4.3658\n",
      "Epoch 60/100\n",
      "12/12 [==============================] - 2s 137ms/step - loss: 4.3244\n",
      "Epoch 61/100\n",
      "12/12 [==============================] - 2s 139ms/step - loss: 4.3037\n",
      "Epoch 62/100\n",
      "12/12 [==============================] - 2s 139ms/step - loss: 4.2747\n",
      "Epoch 63/100\n",
      "12/12 [==============================] - 2s 145ms/step - loss: 4.2461\n",
      "Epoch 64/100\n",
      "12/12 [==============================] - 2s 148ms/step - loss: 4.2025\n",
      "Epoch 65/100\n",
      "12/12 [==============================] - 2s 130ms/step - loss: 4.1842\n",
      "Epoch 66/100\n",
      "12/12 [==============================] - 2s 142ms/step - loss: 4.1472\n",
      "Epoch 67/100\n",
      "12/12 [==============================] - 2s 145ms/step - loss: 4.1164\n",
      "Epoch 68/100\n",
      "12/12 [==============================] - 1s 51ms/step - loss: 4.0822\n",
      "Epoch 69/100\n",
      "12/12 [==============================] - 1s 53ms/step - loss: 4.0493\n",
      "Epoch 70/100\n",
      "12/12 [==============================] - 1s 52ms/step - loss: 4.0156\n",
      "Epoch 71/100\n",
      "12/12 [==============================] - 1s 56ms/step - loss: 3.9998\n",
      "Epoch 72/100\n",
      "12/12 [==============================] - 1s 55ms/step - loss: 3.9653\n",
      "Epoch 73/100\n",
      "12/12 [==============================] - 1s 55ms/step - loss: 3.9351\n",
      "Epoch 74/100\n",
      "12/12 [==============================] - 1s 56ms/step - loss: 3.9042\n",
      "Epoch 75/100\n",
      "12/12 [==============================] - 1s 53ms/step - loss: 3.8908\n",
      "Epoch 76/100\n",
      "12/12 [==============================] - 1s 51ms/step - loss: 3.8598\n",
      "Epoch 77/100\n",
      "12/12 [==============================] - 1s 50ms/step - loss: 3.8221\n",
      "Epoch 78/100\n",
      "12/12 [==============================] - 2s 146ms/step - loss: 3.7938\n",
      "Epoch 79/100\n",
      "12/12 [==============================] - 2s 141ms/step - loss: 3.7634\n",
      "Epoch 80/100\n",
      "12/12 [==============================] - 2s 144ms/step - loss: 3.7411\n",
      "Epoch 81/100\n",
      "12/12 [==============================] - 2s 143ms/step - loss: 3.7133\n",
      "Epoch 82/100\n",
      "12/12 [==============================] - 1s 103ms/step - loss: 3.6890\n",
      "Epoch 83/100\n",
      "12/12 [==============================] - 1s 55ms/step - loss: 3.6590\n",
      "Epoch 84/100\n",
      "12/12 [==============================] - 1s 50ms/step - loss: 3.6270\n",
      "Epoch 85/100\n",
      "12/12 [==============================] - 1s 46ms/step - loss: 3.6071\n",
      "Epoch 86/100\n",
      "12/12 [==============================] - 1s 48ms/step - loss: 3.5765\n",
      "Epoch 87/100\n",
      "12/12 [==============================] - 1s 48ms/step - loss: 3.5614\n",
      "Epoch 88/100\n",
      "12/12 [==============================] - 1s 49ms/step - loss: 3.5257\n",
      "Epoch 89/100\n",
      "12/12 [==============================] - 1s 47ms/step - loss: 3.5044\n",
      "Epoch 90/100\n",
      "12/12 [==============================] - 1s 47ms/step - loss: 3.4710\n",
      "Epoch 91/100\n",
      "12/12 [==============================] - 1s 48ms/step - loss: 3.4428\n",
      "Epoch 92/100\n",
      "12/12 [==============================] - 1s 50ms/step - loss: 3.4283\n",
      "Epoch 93/100\n",
      "12/12 [==============================] - 1s 49ms/step - loss: 3.3959\n",
      "Epoch 94/100\n",
      "12/12 [==============================] - 1s 50ms/step - loss: 3.3687\n",
      "Epoch 95/100\n",
      "12/12 [==============================] - 1s 49ms/step - loss: 3.3446\n",
      "Epoch 96/100\n",
      "12/12 [==============================] - 1s 48ms/step - loss: 3.3142\n",
      "Epoch 97/100\n",
      "12/12 [==============================] - 1s 50ms/step - loss: 3.2804\n",
      "Epoch 98/100\n",
      "12/12 [==============================] - 1s 54ms/step - loss: 3.2580\n",
      "Epoch 99/100\n",
      "12/12 [==============================] - 1s 52ms/step - loss: 3.2430\n",
      "Epoch 100/100\n",
      "12/12 [==============================] - 1s 50ms/step - loss: 3.2156\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x22a1a8af010>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([encoder_input_data , decoder_input_data], decoder_output_data, batch_size=50, epochs=100 ) \n",
    "#model.save( 'model.h5' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0130ae37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_inference_models():\n",
    "    \n",
    "    encoder_model = tf.keras.models.Model(encoder_inputs, encoder_states)\n",
    "    \n",
    "    decoder_state_input_h = tf.keras.layers.Input(shape=( 200 ,))\n",
    "    decoder_state_input_c = tf.keras.layers.Input(shape=( 200 ,))\n",
    "    \n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    \n",
    "    decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "        decoder_embedding , initial_state=decoder_states_inputs)\n",
    "    \n",
    "    decoder_states = [state_h, state_c]\n",
    "\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    \n",
    "    decoder_model = tf.keras.models.Model(\n",
    "        [decoder_inputs] + decoder_states_inputs,\n",
    "        [decoder_outputs] + decoder_states)\n",
    "    \n",
    "    return encoder_model , decoder_model\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f014b57",
   "metadata": {},
   "source": [
    "Talking with the Chatbot\n",
    "define a method str_to_tokens which converts str questions to Integer tokens with padding.\n",
    "\n",
    "First, we take a question as input and predict the state values using enc_model.\n",
    "We set the state values in the decoder's LSTM.\n",
    "Then, we generate a sequence which contains the element.\n",
    "We input this sequence in the dec_model.\n",
    "We replace the element with the element which was predicted by the dec_model and update the state values.\n",
    "We carry out the above steps iteratively till we hit the tag or the maximum answer length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4e0c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_tokens( sentence : str ):\n",
    "\n",
    "    words = sentence.lower().split()\n",
    "    tokens_list = list()\n",
    "  \n",
    "    for word in words:\n",
    "        tokens_list.append( tokenizer.word_index[ word ] ) \n",
    "    return preprocessing.sequence.pad_sequences( [tokens_list] , maxlen=maxlen_questions , padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1f2bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 800ms/step\n",
      "1/1 [==============================] - 1s 740ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      " is one of the science of science favorite science favorite the means favorite the means favorite the most of the os favorite the os favorite the os favorite the os favorite the most of the\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      " is one of the science of science favorite science favorite the means favorite the means favorite the most of the os favorite the os favorite the os favorite the os favorite the most of the\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      " is one of the science of science favorite science favorite the means favorite the means favorite the most of the os favorite the os favorite the os favorite the os favorite the most of the\n"
     ]
    }
   ],
   "source": [
    "enc_model , dec_model = make_inference_models()\n",
    "\n",
    "for _ in range(10):\n",
    "    states_values = enc_model.predict( str_to_tokens( input( 'Enter question : ' ) ) )\n",
    "    empty_target_seq = np.zeros( ( 1 , 1 ) )\n",
    "    empty_target_seq[0, 0] = tokenizer.word_index['start']\n",
    "    stop_condition = False\n",
    "    decoded_translation = ''\n",
    "    while not stop_condition :\n",
    "        dec_outputs , h , c = dec_model.predict([ empty_target_seq ] + states_values )\n",
    "        sampled_word_index = np.argmax( dec_outputs[0, -1, :] )\n",
    "        sampled_word = None\n",
    "        for word , index in tokenizer.word_index.items() :\n",
    "            if sampled_word_index == index :\n",
    "                decoded_translation += ' {}'.format( word )\n",
    "                sampled_word = word\n",
    "        \n",
    "        if sampled_word == 'end' or len(decoded_translation.split()) > maxlen_answers:\n",
    "            stop_condition = True\n",
    "            \n",
    "        empty_target_seq = np.zeros( ( 1 , 1 ) )  \n",
    "        empty_target_seq[ 0 , 0 ] = sampled_word_index\n",
    "        states_values = [ h , c ] \n",
    "\n",
    "    print( decoded_translation )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553240bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
